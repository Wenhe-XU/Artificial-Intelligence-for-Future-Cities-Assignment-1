{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ⅰ -- Math Assignment:\n",
    "## Raw Forward Propagation\n",
    "\n",
    "1. **Input to Hidden Layer Computation:**\n",
    "   - For each neuron in the hidden layer, the input is the dot product of the $\\( x \\)$ vector and the weights $\\( w \\)$, since there is no bias term, so $\\( out = x \\cdot w \\)$.\n",
    "   - Given that the initial weights are 0.1, we have three inputs $\\( x = [6, 2, 2] \\)$.\n",
    "   - Therefore, the $\\( out \\)$ for each neuron in the hidden layer will be $\\( 6 \\times 0.1 + 2 \\times 0.1 + 2 \\times 0.1 = 1.0 \\)$.\n",
    "\n",
    "2. **Activation in Hidden Layer:**\n",
    "   - Apply the ReLU activation function, $\\( \\text{ReLU}(out) = \\max(0, out) \\)$, since $\\( out \\)$ is 1.0, the output after activation remains $1.0$.\n",
    "\n",
    "3. **Hidden to Output Layer Computation:**\n",
    "   - Similarly, the $\\( out \\)$ for the output layer is also the dot product of the inputs, $\\( out = 1.0 \\times 0.1 + 1.0 \\times 0.1 = 0.2 \\)$.\n",
    "\n",
    "4. **Activation in Output Layer:**\n",
    "   - Apply the ReLU activation function to the output layer, $\\( \\text{ReLU}(out) = \\max(0, 0.2) = 0.2 \\)$.\n",
    "   - So, the raw modeled output $\\( \\hat{y} \\)$ is $0.2$.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "To compute the gradient and update the weights, we use the loss function $\\( L = y - \\hat{y} \\)$ and a learning rate of 0.05.\n",
    "\n",
    "1. **Loss Computation:**\n",
    "   - The given true value $\\( y = 0.7 \\)$, the model output $\\( \\hat{y} = 0.2 \\)$.\n",
    "   - The loss $\\( L = y - \\hat{y} = 0.7 - 0.2 = 0.5 \\)$.\n",
    "\n",
    "2. **Gradient for Output Layer Weights:**\n",
    "   - $\\( \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} \\)$.\n",
    "   - $\\( \\frac{\\partial L}{\\partial \\hat{y}} = -1 \\)$ since the loss function is $\\( L = y - \\hat{y} \\)$.\n",
    "   - $\\( \\frac{\\partial \\hat{y}}{\\partial w} \\)$ is the derivative of the ReLU function times the output of the hidden layer. Since ReLU's derivative is a unit step function, for $\\( out = 0.2 \\)$, it is $1$.\n",
    "   - The output of the hidden layer is 1, so $\\( \\frac{\\partial \\hat{y}}{\\partial w} = 1 \\)$.\n",
    "   - Thus, the gradient for the output layer weights $\\( \\frac{\\partial L}{\\partial w} = -1 \\times 1 \\times 1 = -1 \\).$\n",
    "\n",
    "3. **Gradient Calculation for Hidden Layer Weights:**\n",
    "   - The gradient for hidden layer weights (since only one hidden layer neuron outputs a positive value, hence its gradient is non-zero):\n",
    "     $\\( \\frac{\\partial L}{\\partial w_{\\text{hidden}}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\text{out}_{\\text{hidden}}} \\cdot \\frac{\\partial \\text{out}_{\\text{hidden}}}{\\partial w_{\\text{hidden}}} \\)$\n",
    "   - As $\\( \\text{out}_{\\text{hidden}} \\)$ is activated by ReLU, its derivative is 1 for $\\( \\text{out} > 0 \\)$, hence:\n",
    "     $\\( \\frac{\\partial \\hat{y}}{\\partial \\text{out}_{\\text{hidden}}} = w_{\\text{out}} = 0.1 \\)$\n",
    "   - $\\( \\frac{\\partial \\text{out}_{\\text{hidden}}}{\\partial w_{\\text{hidden}}} \\)$ is the input $\\( x \\)$, hence:\n",
    "     $\\( \\frac{\\partial L}{\\partial w_{\\text{hidden}}} = -1 \\times 0.1 \\times x = -1 \\times 0.1 \\times [6, 2, 2] = [-0.6, -0.2, -0.2] \\)$\n",
    "\n",
    "4. **Update Weights:**\n",
    "   - After computing gradients for all weights, update them simultaneously:\n",
    "   - For the output layer weights $\\( w_{\\text{out}} \\)$:\n",
    "     $\\( w_{\\text{out}_{\\text{new}}} = w_{\\text{out}_{\\text{old}}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{out}}} = 0.1 - 0.05 \\times -1 = 0.1 + 0.05 = 0.15 \\)$\n",
    "   - For the hidden layer weights $\\( w_{\\text{hidden}} \\)$, for each corresponding input $\\( x_i \\)$:\n",
    "     $\\( w_{\\text{hidden}_{\\text{new}_6}} = w_{\\text{hidden}_{\\text{old}_6}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{hidden}_6}} = 0.1 - 0.05 \\times -0.6 = 0.1 + 0.03 = 0.13 \\)$\n",
    "     $\\( w_{\\text{hidden}_{\\text{new}_2}}= w_{\\text{hidden}_{\\text{old}_2}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{hidden}_2}} = 0.1 - 0.05 \\times -0.2 = 0.1 + 0.01 = 0.11 \\)$\n",
    "## Updated Forward Propagation\n",
    "\n",
    "1. **Input to Hidden Layer Computation:**\n",
    "   - For each neuron in the hidden layer, the input is the dot product of the $\\( x \\)$ vector and the updated weights $\\( w_{\\text{new}} \\)$, since there is no bias term, so $\\( out = x \\cdot w_{\\text{new}} \\)$.\n",
    "   - Given that the initial weights are 0.1, we have three inputs $\\( x = [6, 2, 2] \\)$.\n",
    "   - Therefore, the $\\( out \\)$ for each neuron in the hidden layer will be $\\( 6 \\times 0.13 + 2 \\times 0.11 + 2 \\times 0.11 = 1.22 \\)$.\n",
    "\n",
    "2. **Activation in Hidden Layer:**\n",
    "   - Apply the ReLU activation function, $\\( \\text{ReLU}(out) = \\max(0, out) \\)$, since $\\( out \\)$ is 1.22, the output after activation remains $1.22$.\n",
    "\n",
    "3. **Hidden to Output Layer Computation:**\n",
    "   - Similarly, the $\\( out \\)$ for the output layer is also the dot product of the inputs, $\\( out = 1.22 \\times 0.15 + 1.22 \\times 0.15 = 0.366 \\)$.\n",
    "\n",
    "4. **Activation in Output Layer:**\n",
    "   - Apply the ReLU activation function to the output layer, $\\( \\text{ReLU}(out) = \\max(0, 0.366) = 0.366 \\)$.\n",
    "   - So, the updated modeled output $\\( \\hat{y}_{\\text{new}} \\)$ is $0.366$.\n",
    "## The Final Answer is [0.366]()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c4d90636983cba5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ⅱ -- Coding Assignment:\n",
    "## a. Data Preprocessing\n",
    "1. Please modify the categorial variables into dummy variables (e.g., season, weathersit, month, hour, weekday)\n",
    "2. Normalize continuous variables using z-score (mean=0,sd=1).\n",
    "3. Exclude the useless features in your training and modeling.\n",
    "4. Separate the training and validation data. Use the last 21 days’ data for\n",
    "validation. Note that the target column is “cnt”. The other two “causal” and\n",
    "“registered” could be overlooked and should not be used as variables in your\n",
    "code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b469780d134d2838"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm \n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:16:17.769006400Z",
     "start_time": "2024-03-19T10:16:17.747111400Z"
    }
   },
   "id": "dcbbc9d179719635"
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "class DataPreProcess(object):\n",
    "    SourceDataPath='bikeRidershipPredictionDataHour.csv'\n",
    "    DataSaveFolder='DataPreProcessed'\n",
    "    # Exclude UnCorrelated Features\n",
    "    UnCorrelatedBar=0.1\n",
    "    # Overlooked features\n",
    "    OverlookedFeatures=['casual','registered']\n",
    "    # The categorial variables\n",
    "    CategoryVariables=['season','yr','mnth','hr','holiday','weekday','workingday','weathersit']\n",
    "    # The continuous variables\n",
    "    ContinuousVariables=['temp','atemp','hum','windspeed']\n",
    "    # The target variable\n",
    "    TargetVariable='cnt'\n",
    "    # Validation data length (last days)\n",
    "    ValidationDays=21\n",
    "    def DropOverlookedVariables(self):\n",
    "        # Drop the useless features\n",
    "        self.RawData.drop(self.OverlookedFeatures,axis=1,inplace=True)\n",
    "    def FileSystemMaker(self):\n",
    "        # Create the folder for saving the preprocessed data\n",
    "        if not os.path.exists(self.DataSaveFolder):\n",
    "            os.makedirs(self.DataSaveFolder)\n",
    "        if self.IFVisualAnalysis:\n",
    "            self.VisualFolder=os.path.join(self.DataSaveFolder,'VisualAnalysisResults')\n",
    "            if not os.path.exists(self.VisualFolder):\n",
    "                os.makedirs(self.VisualFolder)\n",
    "        if self.IfDummy:\n",
    "            self.DummyFolder=os.path.join(self.DataSaveFolder,'Dummy')\n",
    "            if not os.path.exists(self.DummyFolder):\n",
    "                os.makedirs(self.DummyFolder)\n",
    "        if self.IfNormalize:\n",
    "            self.NormalizeFolder=os.path.join(self.DataSaveFolder,'Normalize')\n",
    "            if not os.path.exists(self.NormalizeFolder):\n",
    "                os.makedirs(self.NormalizeFolder)\n",
    "        self.ProcessedDatasetFolder=os.path.join(self.DataSaveFolder,'ProcessedDataset')\n",
    "        if not os.path.exists(self.ProcessedDatasetFolder):\n",
    "            os.makedirs(self.ProcessedDatasetFolder)\n",
    "    def VisualAnalysis(self):\n",
    "        # Visual analysis of the data\n",
    "        for i in tqdm.tqdm(self.CategoryVariables,desc='Category Var Visual Analysis'):\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            sns.boxplot(x = i, y = self.TargetVariable, data = self.RawData)\n",
    "            plt.title('Boxplot of '+i+' vs '+self.TargetVariable)\n",
    "            plt.savefig(os.path.join(self.VisualFolder,i+'.png'))\n",
    "            if self.IFShowVisualResult:\n",
    "                plt.show()\n",
    "        print('The boxplot of the categorial variables have been saved in the folder:',self.VisualFolder)\n",
    "        for i in tqdm.tqdm(self.ContinuousVariables,desc='Continuous Var Visual Analysis'):\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            sns.boxplot(self.RawData[i])\n",
    "            plt.title('Boxplot of '+i)\n",
    "            plt.savefig(os.path.join(self.VisualFolder,i+'.png'))\n",
    "            if self.IFShowVisualResult:\n",
    "                plt.show()\n",
    "        print('The boxplot of the categorial variables have been saved in the folder:',self.VisualFolder)\n",
    "        Varlist=self.ContinuousVariables+[self.TargetVariable]\n",
    "        sns.pairplot(self.RawData[Varlist])\n",
    "        plt.title('Pairplot of Continuous Variables Vs Target Variable')\n",
    "        plt.savefig(os.path.join(self.VisualFolder,'Pairplot of Continuous Variables Vs Target Variable.png'))\n",
    "        if self.IFShowVisualResult:\n",
    "            plt.show()\n",
    "        print('The pairplot of the continuous variables have been saved in the folder:',self.VisualFolder)\n",
    "    def DummyVariables(self):\n",
    "        # Create the dummy variables for the categorial variables\n",
    "        for i in tqdm.tqdm(self.CategoryVariables,desc='Creating Dummy Variables'):\n",
    "            DummyData=pd.get_dummies(self.RawData[i],drop_first=False, prefix=i)\n",
    "            DummyResult=pd.concat([self.RawData,DummyData],axis=1)\n",
    "            # Remove the original categorial variables\n",
    "            Columns=self.RawData.columns.tolist()\n",
    "            # Save the dummy variables and the key variable\n",
    "            Columns.remove('instant')\n",
    "            DummyResult.drop(Columns,axis=1,inplace=True)\n",
    "            DummyResult.to_csv(os.path.join(self.DummyFolder,i+'.csv'),index=False)\n",
    "    def NormalizeVariables(self,target_mean=0,target_sd=1):\n",
    "        # Normalize the continuous variables\n",
    "        for i in tqdm.tqdm(self.ContinuousVariables,desc='Normalizing Variables'):\n",
    "            Mean=self.RawData[i].mean()\n",
    "            SD=self.RawData[i].std()\n",
    "            NormalizedData=(self.RawData[i]-Mean)/SD\n",
    "            NormalizedData=NormalizedData*target_sd+target_mean\n",
    "            NormalizedData=pd.concat([self.RawData['instant'],NormalizedData],axis=1)\n",
    "            NormalizedData.to_csv(os.path.join(self.NormalizeFolder,i+'.csv'),index=False)\n",
    "    def ExcludeUselessFeatures(self):\n",
    "        # Exclude the useless features\n",
    "        self.UselessFeatureList=[]\n",
    "        for i in self.CategoryVariables:\n",
    "            corr=self.RawData[[i,self.TargetVariable]].corr()[self.TargetVariable][i]\n",
    "            print('The correlation between',i,'and',self.TargetVariable,'is:',corr)\n",
    "            if abs(corr)<self.UnCorrelatedBar:\n",
    "                self.UselessFeatureList.append(i)\n",
    "        for i in self.ContinuousVariables:\n",
    "            corr=self.RawData[[i,self.TargetVariable]].corr()[self.TargetVariable][i]\n",
    "            print('The correlation between',i,'and',self.TargetVariable,'is:',corr)\n",
    "            if abs(corr)<self.UnCorrelatedBar:\n",
    "                self.UselessFeatureList.append(i)\n",
    "        print('The bar for uncorrelated features is:',self.UnCorrelatedBar)        \n",
    "        print('The features:',self.UselessFeatureList,'are uncorrelated with the target variable:',self.TargetVariable)\n",
    "        self.RawData.drop(self.UselessFeatureList,axis=1,inplace=True)\n",
    "        print('The uncorrelated features have been dropped.')\n",
    "    def SeparateData(self):\n",
    "        # Update the data\n",
    "        # Update the data\n",
    "        Date=pd.to_datetime(self.RawData['dteday'].unique()).strftime('%Y-%m-%d')\n",
    "        Date=Date.sort_values(ascending=False)\n",
    "        ValidationDate=[]\n",
    "        print('Rawdata is form:',Date[0],'to',Date[-1],'totally',len(Date),'days.')\n",
    "        for i in range(0,self.ValidationDays):\n",
    "            ValidationDate.append(Date[i])\n",
    "        self.ValidationData=self.RawData[self.RawData['dteday'].isin(ValidationDate)]\n",
    "        self.ValidationData.drop('dteday',axis=1,inplace=True)\n",
    "        print('The validation data is from:',ValidationDate[0],'to',ValidationDate[-1],'totally',len(ValidationDate),'days','with',len(self.ValidationData),'records.')\n",
    "        TrainDate=Date[self.ValidationDays:]\n",
    "        self.TrainData=self.RawData[self.RawData['dteday'].isin(TrainDate)]\n",
    "        self.TrainData.drop('dteday',axis=1,inplace=True)\n",
    "        print('The training data is from:',TrainDate[0],'to',TrainDate[-1],'totally',len(TrainDate),'days','with',len(self.TrainData),'records.')\n",
    "    def CreatDataset(self):\n",
    "        # Create the dataset\n",
    "        self.SeparateData()\n",
    "        for i in self.CategoryVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                DummyData=pd.read_csv(os.path.join(self.DummyFolder,i+'.csv'))\n",
    "                self.ValidationData.drop(i,axis=1,inplace=True)\n",
    "                self.ValidationData=pd.merge(self.ValidationData,DummyData,on='instant',how='left')\n",
    "        for i in self.ContinuousVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                NormalizedData=pd.read_csv(os.path.join(self.NormalizeFolder,i+'.csv'))\n",
    "                self.ValidationData.drop(i,axis=1,inplace=True)\n",
    "                self.ValidationData=pd.merge(self.ValidationData,NormalizedData,on='instant',how='left')\n",
    "        for i in self.CategoryVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                DummyData=pd.read_csv(os.path.join(self.DummyFolder,i+'.csv'))\n",
    "                self.TrainData.drop(i,axis=1,inplace=True)\n",
    "                self.TrainData=pd.merge(self.TrainData,DummyData,on='instant',how='left')\n",
    "        for i in self.ContinuousVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                NormalizedData=pd.read_csv(os.path.join(self.NormalizeFolder,i+'.csv'))\n",
    "                self.TrainData.drop(i,axis=1,inplace=True)\n",
    "                self.TrainData=pd.merge(self.TrainData,NormalizedData,on='instant',how='left')\n",
    "        np.save(os.path.join(self.ProcessedDatasetFolder,'ValidationData.npy'),self.ValidationData)\n",
    "        np.save(os.path.join(self.ProcessedDatasetFolder,'TrainData.npy'),self.TrainData)\n",
    "        print('The preprocessed data has been saved in the folder:',self.ProcessedDatasetFolder)\n",
    "        print('The preprocessed data has been saved as ValidationData.npy and TrainData.npy.')\n",
    "    def __init__(self,IFVisualAnalysis=False,IFShowVisualResult=False,IfCheckInfo=False,IfDummy=True,IfNormalize=True):\n",
    "        # Read the source data\n",
    "        self.RawData=pd.read_csv(self.SourceDataPath)\n",
    "        # Drop the overlooked features\n",
    "        self.DropOverlookedVariables()\n",
    "        print('The overlooked features:',self.OverlookedFeatures,' have been dropped.')\n",
    "        # Check the basic information of the source data\n",
    "        if IfCheckInfo:\n",
    "            self.RawData.info()\n",
    "        # Set the parameters for preprocessing\n",
    "        self.IFVisualAnalysis=IFVisualAnalysis\n",
    "        self.IFShowVisualResult=IFShowVisualResult\n",
    "        self.IfDummy=IfDummy\n",
    "        self.IfNormalize=IfNormalize\n",
    "        # Create the folder for saving the preprocessed data\n",
    "        self.FileSystemMaker()\n",
    "        if self.IFVisualAnalysis:\n",
    "            print('The visual analysis results have been saved in the folder:',self.VisualFolder)\n",
    "            self.VisualAnalysis()\n",
    "        if self.IfDummy:\n",
    "            print('The values of the categorial variables are:',self.CategoryVariables)\n",
    "            self.DummyVariables()\n",
    "            print('The dummy variables have been created.')\n",
    "            print('The dummy variables have been saved in the folder:',self.DummyFolder)\n",
    "        if self.IfNormalize:\n",
    "            print('The values of the continuous variables are:',self.ContinuousVariables)\n",
    "            self.NormalizeVariables()\n",
    "            print('The continuous variables have been normalized.')\n",
    "            print('The normalized data has been saved in the folder:',self.NormalizeFolder)\n",
    "        self.ExcludeUselessFeatures()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:25:17.462501700Z",
     "start_time": "2024-03-19T10:25:17.455408500Z"
    }
   },
   "id": "db6d2cccb2023978"
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overlooked features: ['casual', 'registered']  have been dropped.\n",
      "The values of the categorial variables are: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Dummy Variables: 100%|██████████| 8/8 [00:00<00:00, 57.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dummy variables have been created.\n",
      "The dummy variables have been saved in the folder: DataPreProcessed\\Dummy\n",
      "The values of the continuous variables are: ['temp', 'atemp', 'hum', 'windspeed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Variables: 100%|██████████| 4/4 [00:00<00:00, 58.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The continuous variables have been normalized.\n",
      "The normalized data has been saved in the folder: DataPreProcessed\\Normalize\n",
      "The correlation between season and cnt is: 0.17805573098267663\n",
      "The correlation between yr and cnt is: 0.2504948988596485\n",
      "The correlation between mnth and cnt is: 0.12063776021315144\n",
      "The correlation between hr and cnt is: 0.39407149778294204\n",
      "The correlation between holiday and cnt is: -0.030927303249110614\n",
      "The correlation between weekday and cnt is: 0.02689985999083953\n",
      "The correlation between workingday and cnt is: 0.030284367747910722\n",
      "The correlation between weathersit and cnt is: -0.14242613813809568\n",
      "The correlation between temp and cnt is: 0.4047722757786578\n",
      "The correlation between atemp and cnt is: 0.4009293041266357\n",
      "The correlation between hum and cnt is: -0.32291074082456017\n",
      "The correlation between windspeed and cnt is: 0.09323378392612537\n",
      "The bar for uncorrelated features is: 0.1\n",
      "The features: ['holiday', 'weekday', 'workingday', 'windspeed'] are uncorrelated with the target variable: cnt\n",
      "The uncorrelated features have been dropped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the basic information of the source data\n",
    "DataPreProcess=DataPreProcess(IFVisualAnalysis=False,IFShowVisualResult=False,IfCheckInfo=False,IfDummy=True,IfNormalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:25:18.685596800Z",
     "start_time": "2024-03-19T10:25:18.448051600Z"
    }
   },
   "id": "65f5c27004fa6782"
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rawdata is form: 2012-12-31 to 2011-01-01 totally 731 days.\n",
      "The validation data is from: 2012-12-31 to 2012-12-11 totally 21 days with 502 records.\n",
      "The training data is from: 2012-12-10 to 2011-01-01 totally 710 days with 16877 records.\n",
      "The preprocessed data has been saved in the folder: DataPreProcessed\\ProcessedDataset\n",
      "The preprocessed data has been saved as ValidationData.npy and TrainData.npy.\n"
     ]
    }
   ],
   "source": [
    "DataPreProcess.CreatDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:25:34.572108700Z",
     "start_time": "2024-03-19T10:25:34.395353700Z"
    }
   },
   "id": "bbf8d2d0034be650"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "db82341ead305ce4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

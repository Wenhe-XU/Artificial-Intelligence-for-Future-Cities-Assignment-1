{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Ⅰ -- Math Assignment:\n",
    "## Raw Forward Propagation\n",
    "\n",
    "1. **Input to Hidden Layer Computation:**\n",
    "   - For each neuron in the hidden layer, the input is the dot product of the $\\( x \\)$ vector and the weights $\\( w \\)$, since there is no bias term, so $\\( out = x \\cdot w \\)$.\n",
    "   - Given that the initial weights are 0.1, we have three inputs $\\( x = [6, 2, 2] \\)$.\n",
    "   - Therefore, the $\\( out \\)$ for each neuron in the hidden layer will be $\\( 6 \\times 0.1 + 2 \\times 0.1 + 2 \\times 0.1 = 1.0 \\)$.\n",
    "\n",
    "2. **Activation in Hidden Layer:**\n",
    "   - Apply the ReLU activation function, $\\( \\text{ReLU}(out) = \\max(0, out) \\)$, since $\\( out \\)$ is 1.0, the output after activation remains $1.0$.\n",
    "\n",
    "3. **Hidden to Output Layer Computation:**\n",
    "   - Similarly, the $\\( out \\)$ for the output layer is also the dot product of the inputs, $\\( out = 1.0 \\times 0.1 + 1.0 \\times 0.1 = 0.2 \\)$.\n",
    "\n",
    "4. **Activation in Output Layer:**\n",
    "   - Apply the ReLU activation function to the output layer, $\\( \\text{ReLU}(out) = \\max(0, 0.2) = 0.2 \\)$.\n",
    "   - So, the raw modeled output $\\( \\hat{y} \\)$ is $0.2$.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "To compute the gradient and update the weights, we use the loss function $\\( L = y - \\hat{y} \\)$ and a learning rate of 0.05.\n",
    "\n",
    "1. **Loss Computation:**\n",
    "   - The given true value $\\( y = 0.7 \\)$, the model output $\\( \\hat{y} = 0.2 \\)$.\n",
    "   - The loss $\\( L = y - \\hat{y} = 0.7 - 0.2 = 0.5 \\)$.\n",
    "\n",
    "2. **Gradient for Output Layer Weights:**\n",
    "   - $\\( \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} \\)$.\n",
    "   - $\\( \\frac{\\partial L}{\\partial \\hat{y}} = -1 \\)$ since the loss function is $\\( L = y - \\hat{y} \\)$.\n",
    "   - $\\( \\frac{\\partial \\hat{y}}{\\partial w} \\)$ is the derivative of the ReLU function times the output of the hidden layer. Since ReLU's derivative is a unit step function, for $\\( out = 0.2 \\)$, it is $1$.\n",
    "   - The output of the hidden layer is 1, so $\\( \\frac{\\partial \\hat{y}}{\\partial w} = 1 \\)$.\n",
    "   - Thus, the gradient for the output layer weights $\\( \\frac{\\partial L}{\\partial w} = -1 \\times 1 \\times 1 = -1 \\).$\n",
    "\n",
    "3. **Gradient Calculation for Hidden Layer Weights:**\n",
    "   - The gradient for hidden layer weights (since only one hidden layer neuron outputs a positive value, hence its gradient is non-zero):\n",
    "     $\\( \\frac{\\partial L}{\\partial w_{\\text{hidden}}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\text{out}_{\\text{hidden}}} \\cdot \\frac{\\partial \\text{out}_{\\text{hidden}}}{\\partial w_{\\text{hidden}}} \\)$\n",
    "   - As $\\( \\text{out}_{\\text{hidden}} \\)$ is activated by ReLU, its derivative is 1 for $\\( \\text{out} > 0 \\)$, hence:\n",
    "     $\\( \\frac{\\partial \\hat{y}}{\\partial \\text{out}_{\\text{hidden}}} = w_{\\text{out}} = 0.1 \\)$\n",
    "   - $\\( \\frac{\\partial \\text{out}_{\\text{hidden}}}{\\partial w_{\\text{hidden}}} \\)$ is the input $\\( x \\)$, hence:\n",
    "     $\\( \\frac{\\partial L}{\\partial w_{\\text{hidden}}} = -1 \\times 0.1 \\times x = -1 \\times 0.1 \\times [6, 2, 2] = [-0.6, -0.2, -0.2] \\)$\n",
    "\n",
    "4. **Update Weights:**\n",
    "   - After computing gradients for all weights, update them simultaneously:\n",
    "   - For the output layer weights $\\( w_{\\text{out}} \\)$:\n",
    "     $\\( w_{\\text{out}_{\\text{new}}} = w_{\\text{out}_{\\text{old}}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{out}}} = 0.1 - 0.05 \\times -1 = 0.1 + 0.05 = 0.15 \\)$\n",
    "   - For the hidden layer weights $\\( w_{\\text{hidden}} \\)$, for each corresponding input $\\( x_i \\)$:\n",
    "     $\\( w_{\\text{hidden}_{\\text{new}_6}} = w_{\\text{hidden}_{\\text{old}_6}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{hidden}_6}} = 0.1 - 0.05 \\times -0.6 = 0.1 + 0.03 = 0.13 \\)$\n",
    "     $\\( w_{\\text{hidden}_{\\text{new}_2}}= w_{\\text{hidden}_{\\text{old}_2}} - \\alpha \\times \\frac{\\partial L}{\\partial w_{\\text{hidden}_2}} = 0.1 - 0.05 \\times -0.2 = 0.1 + 0.01 = 0.11 \\)$\n",
    "## Updated Forward Propagation\n",
    "\n",
    "1. **Input to Hidden Layer Computation:**\n",
    "   - For each neuron in the hidden layer, the input is the dot product of the $\\( x \\)$ vector and the updated weights $\\( w_{\\text{new}} \\)$, since there is no bias term, so $\\( out = x \\cdot w_{\\text{new}} \\)$.\n",
    "   - Given that the initial weights are 0.1, we have three inputs $\\( x = [6, 2, 2] \\)$.\n",
    "   - Therefore, the $\\( out \\)$ for each neuron in the hidden layer will be $\\( 6 \\times 0.13 + 2 \\times 0.11 + 2 \\times 0.11 = 1.22 \\)$.\n",
    "\n",
    "2. **Activation in Hidden Layer:**\n",
    "   - Apply the ReLU activation function, $\\( \\text{ReLU}(out) = \\max(0, out) \\)$, since $\\( out \\)$ is 1.22, the output after activation remains $1.22$.\n",
    "\n",
    "3. **Hidden to Output Layer Computation:**\n",
    "   - Similarly, the $\\( out \\)$ for the output layer is also the dot product of the inputs, $\\( out = 1.22 \\times 0.15 + 1.22 \\times 0.15 = 0.366 \\)$.\n",
    "\n",
    "4. **Activation in Output Layer:**\n",
    "   - Apply the ReLU activation function to the output layer, $\\( \\text{ReLU}(out) = \\max(0, 0.366) = 0.366 \\)$.\n",
    "   - So, the updated modeled output $\\( \\hat{y}_{\\text{new}} \\)$ is $0.366$.\n",
    "## The Final Answer is [0.366]()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c4d90636983cba5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ⅱ -- Coding Assignment:\n",
    "## a. Data Preprocessing\n",
    "1. Please modify the categorial variables into dummy variables (e.g., season, weathersit, month, hour, weekday)\n",
    "2. Normalize continuous variables using z-score (mean=0,sd=1).\n",
    "3. Exclude the useless features in your training and modeling.\n",
    "4. Separate the training and validation data. Use the last 21 days’ data for\n",
    "validation. Note that the target column is “cnt”. The other two “causal” and\n",
    "“registered” could be overlooked and should not be used as variables in your\n",
    "code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b469780d134d2838"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm \n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:16:17.769006400Z",
     "start_time": "2024-03-19T10:16:17.747111400Z"
    }
   },
   "id": "dcbbc9d179719635"
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "class DataPreProcess(object):\n",
    "    SourceDataPath='bikeRidershipPredictionDataHour.csv'\n",
    "    DataSaveFolder='DataPreProcessed'\n",
    "    # Exclude UnCorrelated Features\n",
    "    UnCorrelatedBar=0.1\n",
    "    # Overlooked features\n",
    "    OverlookedFeatures=['casual','registered']\n",
    "    # The categorial variables\n",
    "    CategoryVariables=['season','yr','mnth','hr','holiday','weekday','workingday','weathersit']\n",
    "    # The continuous variables\n",
    "    ContinuousVariables=['temp','atemp','hum','windspeed']\n",
    "    # The target variable\n",
    "    TargetVariable='cnt'\n",
    "    # Validation data length (last days)\n",
    "    ValidationDays=21\n",
    "    def DropOverlookedVariables(self):\n",
    "        # Drop the useless features\n",
    "        self.RawData.drop(self.OverlookedFeatures,axis=1,inplace=True)\n",
    "    def FileSystemMaker(self):\n",
    "        # Create the folder for saving the preprocessed data\n",
    "        if not os.path.exists(self.DataSaveFolder):\n",
    "            os.makedirs(self.DataSaveFolder)\n",
    "        self.VisualFolder=os.path.join(self.DataSaveFolder,'VisualAnalysisResults')\n",
    "        if not os.path.exists(self.VisualFolder):\n",
    "            os.makedirs(self.VisualFolder)\n",
    "        self.DummyFolder=os.path.join(self.DataSaveFolder,'Dummy')\n",
    "        if not os.path.exists(self.DummyFolder):\n",
    "            os.makedirs(self.DummyFolder)\n",
    "        self.NormalizeFolder=os.path.join(self.DataSaveFolder,'Normalize')\n",
    "        if not os.path.exists(self.NormalizeFolder):\n",
    "            os.makedirs(self.NormalizeFolder)\n",
    "        self.ProcessedDatasetFolder=os.path.join(self.DataSaveFolder,'ProcessedDataset')\n",
    "        if not os.path.exists(self.ProcessedDatasetFolder):\n",
    "            os.makedirs(self.ProcessedDatasetFolder)\n",
    "    def VisualAnalysis(self):\n",
    "        # Visual analysis of the data\n",
    "        for i in tqdm.tqdm(self.CategoryVariables,desc='Category Var Visual Analysis'):\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            sns.boxplot(x = i, y = self.TargetVariable, data = self.RawData)\n",
    "            plt.title('Boxplot of '+i+' vs '+self.TargetVariable)\n",
    "            plt.savefig(os.path.join(self.VisualFolder,i+'.png'))\n",
    "            if self.IFShowVisualResult:\n",
    "                plt.show()\n",
    "        print('The boxplot of the categorial variables have been saved in the folder:',self.VisualFolder)\n",
    "        for i in tqdm.tqdm(self.ContinuousVariables,desc='Continuous Var Visual Analysis'):\n",
    "            plt.figure(figsize=(20, 12))\n",
    "            sns.boxplot(self.RawData[i])\n",
    "            plt.title('Boxplot of '+i)\n",
    "            plt.savefig(os.path.join(self.VisualFolder,i+'.png'))\n",
    "            if self.IFShowVisualResult:\n",
    "                plt.show()\n",
    "        print('The boxplot of the categorial variables have been saved in the folder:',self.VisualFolder)\n",
    "        Varlist=self.ContinuousVariables+[self.TargetVariable]\n",
    "        sns.pairplot(self.RawData[Varlist])\n",
    "        plt.title('Pairplot of Continuous Variables Vs Target Variable')\n",
    "        plt.savefig(os.path.join(self.VisualFolder,'Pairplot of Continuous Variables Vs Target Variable.png'))\n",
    "        if self.IFShowVisualResult:\n",
    "            plt.show()\n",
    "        print('The pairplot of the continuous variables have been saved in the folder:',self.VisualFolder)\n",
    "    def DummyVariables(self):\n",
    "        # Create the dummy variables for the categorial variables\n",
    "        for i in tqdm.tqdm(self.CategoryVariables,desc='Creating Dummy Variables'):\n",
    "            DummyData=pd.get_dummies(self.RawData[i],drop_first=False, prefix=i)\n",
    "            DummyResult=pd.concat([self.RawData,DummyData],axis=1)\n",
    "            # Remove the original categorial variables\n",
    "            Columns=self.RawData.columns.tolist()\n",
    "            # Save the dummy variables and the key variable\n",
    "            Columns.remove('instant')\n",
    "            DummyResult.drop(Columns,axis=1,inplace=True)\n",
    "            DummyResult.to_csv(os.path.join(self.DummyFolder,i+'.csv'),index=False)\n",
    "    def NormalizeVariables(self,target_mean=0,target_sd=1):\n",
    "        # Normalize the continuous variables\n",
    "        for i in tqdm.tqdm(self.ContinuousVariables,desc='Normalizing Variables'):\n",
    "            Mean=self.RawData[i].mean()\n",
    "            SD=self.RawData[i].std()\n",
    "            NormalizedData=(self.RawData[i]-Mean)/SD\n",
    "            NormalizedData=NormalizedData*target_sd+target_mean\n",
    "            NormalizedData=pd.concat([self.RawData['instant'],NormalizedData],axis=1)\n",
    "            NormalizedData.to_csv(os.path.join(self.NormalizeFolder,i+'.csv'),index=False)\n",
    "    def ExcludeUselessFeatures(self):\n",
    "        # Exclude the useless features\n",
    "        self.UselessFeatureList=[]\n",
    "        for i in self.CategoryVariables:\n",
    "            corr=self.RawData[[i,self.TargetVariable]].corr()[self.TargetVariable][i]\n",
    "            print('The correlation between',i,'and',self.TargetVariable,'is:',corr)\n",
    "            if abs(corr)<self.UnCorrelatedBar:\n",
    "                self.UselessFeatureList.append(i)\n",
    "        for i in self.ContinuousVariables:\n",
    "            corr=self.RawData[[i,self.TargetVariable]].corr()[self.TargetVariable][i]\n",
    "            print('The correlation between',i,'and',self.TargetVariable,'is:',corr)\n",
    "            if abs(corr)<self.UnCorrelatedBar:\n",
    "                self.UselessFeatureList.append(i)\n",
    "        print('The bar for uncorrelated features is:',self.UnCorrelatedBar)        \n",
    "        print('The features:',self.UselessFeatureList,'are uncorrelated with the target variable:',self.TargetVariable)\n",
    "        self.RawData.drop(self.UselessFeatureList,axis=1,inplace=True)\n",
    "        print('The uncorrelated features have been dropped.')\n",
    "    def SeparateData(self):\n",
    "        # Separate the training and validation data\n",
    "        Date=pd.to_datetime(self.RawData['dteday'].unique()).strftime('%Y-%m-%d')\n",
    "        Date=Date.sort_values(ascending=False)\n",
    "        ValidationDate=[]\n",
    "        print('Rawdata is form:',Date[0],'to',Date[-1],'totally',len(Date),'days.')\n",
    "        for i in range(0,self.ValidationDays):\n",
    "            ValidationDate.append(Date[i])\n",
    "        self.ValidationData=self.RawData[self.RawData['dteday'].isin(ValidationDate)]\n",
    "        self.ValidationData.drop('dteday',axis=1,inplace=True)\n",
    "        print('The validation data is from:',ValidationDate[0],'to',ValidationDate[-1],'totally',len(ValidationDate),'days','with',len(self.ValidationData),'records.')\n",
    "        TrainDate=Date[self.ValidationDays:]\n",
    "        self.TrainData=self.RawData[self.RawData['dteday'].isin(TrainDate)]\n",
    "        self.TrainData.drop('dteday',axis=1,inplace=True)\n",
    "        print('The training data is from:',TrainDate[0],'to',TrainDate[-1],'totally',len(TrainDate),'days','with',len(self.TrainData),'records.')\n",
    "    def CreatDataset(self):\n",
    "        # Create the dataset\n",
    "        self.SeparateData()\n",
    "        NotCreate=False\n",
    "        for i in self.CategoryVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                if not os.path.exists(os.path.join(self.DummyFolder,i+'.csv')):\n",
    "                    print('The dummy variable:',i+'.csv','does not exist.','Please set IfDummy=True.')\n",
    "                    NotCreate=True\n",
    "                    break\n",
    "                DummyData=pd.read_csv(os.path.join(self.DummyFolder,i+'.csv'))\n",
    "                self.ValidationData.drop(i,axis=1,inplace=True)\n",
    "                self.ValidationData=pd.merge(self.ValidationData,DummyData,on='instant',how='left')\n",
    "        for i in self.ContinuousVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                if not os.path.exists(os.path.join(self.NormalizeFolder,i+'.csv')):\n",
    "                    print('The normalized variable:',i+'.csv','does not exist.','Please set IfNormalize=True.')\n",
    "                    NotCreate=True\n",
    "                    break\n",
    "                NormalizedData=pd.read_csv(os.path.join(self.NormalizeFolder,i+'.csv'))\n",
    "                self.ValidationData.drop(i,axis=1,inplace=True)\n",
    "                self.ValidationData=pd.merge(self.ValidationData,NormalizedData,on='instant',how='left')\n",
    "        for i in self.CategoryVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                if not os.path.exists(os.path.join(self.DummyFolder,i+'.csv')):\n",
    "                    print('The dummy variable:',i+'.csv','does not exist.','Please set IfDummy=True.')\n",
    "                    NotCreate=True\n",
    "                    break\n",
    "                DummyData=pd.read_csv(os.path.join(self.DummyFolder,i+'.csv'))\n",
    "                self.TrainData.drop(i,axis=1,inplace=True)\n",
    "                self.TrainData=pd.merge(self.TrainData,DummyData,on='instant',how='left')\n",
    "        for i in self.ContinuousVariables:\n",
    "            if (i not in self.UselessFeatureList):\n",
    "                if not os.path.exists(os.path.join(self.NormalizeFolder,i+'.csv')):\n",
    "                    print('The normalized variable:',i+'.csv','does not exist.','Please set IfNormalize=True.')\n",
    "                    NotCreate=True\n",
    "                    break\n",
    "                NormalizedData=pd.read_csv(os.path.join(self.NormalizeFolder,i+'.csv'))\n",
    "                self.TrainData.drop(i,axis=1,inplace=True)\n",
    "                self.TrainData=pd.merge(self.TrainData,NormalizedData,on='instant',how='left')\n",
    "        # Save the preprocessed data\n",
    "        if NotCreate:\n",
    "            print('The preprocessed data has not been saved.')\n",
    "            return\n",
    "        else:\n",
    "            np.save(os.path.join(self.ProcessedDatasetFolder,'ValidationData.npy'),self.ValidationData)\n",
    "            np.save(os.path.join(self.ProcessedDatasetFolder,'TrainData.npy'),self.TrainData)\n",
    "            print('The preprocessed data has been saved in the folder:',self.ProcessedDatasetFolder)\n",
    "            print('The preprocessed data has been saved as ValidationData.npy and TrainData.npy.')\n",
    "    def __init__(self,IFVisualAnalysis=False,IFShowVisualResult=False,IfCheckInfo=False,IfDummy=True,IfNormalize=True):\n",
    "        # Read the source data\n",
    "        self.RawData=pd.read_csv(self.SourceDataPath)\n",
    "        # Drop the overlooked features\n",
    "        self.DropOverlookedVariables()\n",
    "        print('The overlooked features:',self.OverlookedFeatures,' have been dropped.')\n",
    "        # Check the basic information of the source data\n",
    "        if IfCheckInfo:\n",
    "            self.RawData.info()\n",
    "        # Set the parameters for preprocessing\n",
    "        self.IFVisualAnalysis=IFVisualAnalysis\n",
    "        self.IFShowVisualResult=IFShowVisualResult\n",
    "        self.IfDummy=IfDummy\n",
    "        self.IfNormalize=IfNormalize\n",
    "        # Create the folder for saving the preprocessed data\n",
    "        self.FileSystemMaker()\n",
    "        if self.IFVisualAnalysis:\n",
    "            print('The visual analysis results have been saved in the folder:',self.VisualFolder)\n",
    "            self.VisualAnalysis()\n",
    "        if self.IfDummy:\n",
    "            print('The values of the categorial variables are:',self.CategoryVariables)\n",
    "            self.DummyVariables()\n",
    "            print('The dummy variables have been created.')\n",
    "            print('The dummy variables have been saved in the folder:',self.DummyFolder)\n",
    "        if self.IfNormalize:\n",
    "            print('The values of the continuous variables are:',self.ContinuousVariables)\n",
    "            self.NormalizeVariables()\n",
    "            print('The continuous variables have been normalized.')\n",
    "            print('The normalized data has been saved in the folder:',self.NormalizeFolder)\n",
    "        self.ExcludeUselessFeatures()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:40:51.631286700Z",
     "start_time": "2024-03-19T10:40:51.614569Z"
    }
   },
   "id": "db6d2cccb2023978"
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overlooked features: ['casual', 'registered']  have been dropped.\n",
      "The values of the categorial variables are: ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Dummy Variables: 100%|██████████| 8/8 [00:00<00:00, 57.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dummy variables have been created.\n",
      "The dummy variables have been saved in the folder: DataPreProcessed\\Dummy\n",
      "The values of the continuous variables are: ['temp', 'atemp', 'hum', 'windspeed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Variables: 100%|██████████| 4/4 [00:00<00:00, 54.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The continuous variables have been normalized.\n",
      "The normalized data has been saved in the folder: DataPreProcessed\\Normalize\n",
      "The correlation between season and cnt is: 0.17805573098267663\n",
      "The correlation between yr and cnt is: 0.2504948988596485\n",
      "The correlation between mnth and cnt is: 0.12063776021315144\n",
      "The correlation between hr and cnt is: 0.39407149778294204\n",
      "The correlation between holiday and cnt is: -0.030927303249110614\n",
      "The correlation between weekday and cnt is: 0.02689985999083953\n",
      "The correlation between workingday and cnt is: 0.030284367747910722\n",
      "The correlation between weathersit and cnt is: -0.14242613813809568\n",
      "The correlation between temp and cnt is: 0.4047722757786578\n",
      "The correlation between atemp and cnt is: 0.4009293041266357\n",
      "The correlation between hum and cnt is: -0.32291074082456017\n",
      "The correlation between windspeed and cnt is: 0.09323378392612537\n",
      "The bar for uncorrelated features is: 0.1\n",
      "The features: ['holiday', 'weekday', 'workingday', 'windspeed'] are uncorrelated with the target variable: cnt\n",
      "The uncorrelated features have been dropped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The default values are all False.\n",
    "# If you want to check the basic information of the source data, please set IfCheckInfo=True.\n",
    "# If you want to visualize the data, please set IFVisualAnalysis=True.\n",
    "# If you want to create the dummy variables, please set IfDummy=True.\n",
    "# If you want to normalize the continuous variables, please set IfNormalize=True.\n",
    "# If you have run the code before, you could set the parameters as False to avoid the repeated operations.\n",
    "# If it comes up with the error: the object is not callable, please restart the kernel and run the code again.\n",
    "DataPreProcess=DataPreProcess(IFVisualAnalysis=False,IFShowVisualResult=False,IfCheckInfo=False,IfDummy=True,IfNormalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:40:52.143735400Z",
     "start_time": "2024-03-19T10:40:51.897437700Z"
    }
   },
   "id": "65f5c27004fa6782"
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rawdata is form: 2012-12-31 to 2011-01-01 totally 731 days.\n",
      "The validation data is from: 2012-12-31 to 2012-12-11 totally 21 days with 502 records.\n",
      "The training data is from: 2012-12-10 to 2011-01-01 totally 710 days with 16877 records.\n",
      "The preprocessed data has been saved in the folder: DataPreProcessed\\ProcessedDataset\n",
      "The preprocessed data has been saved as ValidationData.npy and TrainData.npy.\n"
     ]
    }
   ],
   "source": [
    "# If you want to create the dataset, please run the following code.\n",
    "# If it comes up with the error: \"The dummy variable:xxxx.csv does not exist. Please set IfDummy=True.\", please set IfDummy=True.And create the object again.\n",
    "DataPreProcess.CreatDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T10:40:54.324712200Z",
     "start_time": "2024-03-19T10:40:54.148908200Z"
    }
   },
   "id": "bbf8d2d0034be650"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b. Model Building\n",
    "1. You can copy many lines of codes from the lecture material, but I have left\n",
    "some parts for you to google/figure out on your own. For example, figure out\n",
    "how to feed the pandas tabular data into Pytorch dataloader yourself. \n",
    "2. Try not to copy everything in the lecture notebook, you may have error. \n",
    "3. Evaluate your validation results using R-square and MSE. d. Try different parameters/network structure/optimizers etc., and see if you can\n",
    "obtain good validation results. The number of hidden layers should be equal to\n",
    "or less than"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cbd54be2fccfd8b"
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:29:53.994572Z",
     "start_time": "2024-03-19T12:29:53.980102300Z"
    }
   },
   "id": "e272eae9d7c622f6"
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "n_epochs=200\n",
    "OutputDim=1\n",
    "HiddenDim_1=64\n",
    "HiddenDim_2=32\n",
    "LearningRate=0.001\n",
    "batch_size=32\n",
    "EarlyStoppingPatience=15\n",
    "EarlyStoppingDelta=0.0001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:53.966054600Z",
     "start_time": "2024-03-19T12:50:53.958691500Z"
    }
   },
   "id": "df38b9a67214599"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LossList=[]\n",
    "ValidationLossList=[]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16dab52d61f05f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "outputs": [],
   "source": [
    "class BikeRidershipDataset(Dataset):\n",
    "    # Create the dataset based on the preprocessed data,inheriting from the Dataset class in PyTorch\n",
    "    def __init__(self,DataPath):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        # Load the preprocessed data\n",
    "        self.data=np.load(DataPath,allow_pickle=True)\n",
    "        # Get the features and the target variable\n",
    "        self.features=self.data[:,2:]\n",
    "        self.targets=self.data[:,1]\n",
    "        self.targets=self.scaler.fit_transform(self.targets.reshape(-1,1))\n",
    "        self.features=np.asarray(self.features,dtype=np.float32)\n",
    "        self.targets=np.asarray(self.targets,dtype=np.float32)\n",
    "        self.features=torch.from_numpy(self.features)\n",
    "        self.targets=torch.from_numpy(self.targets).view(-1,1)\n",
    "    # Get the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    # Get the features and the target variable\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:54.355372400Z",
     "start_time": "2024-03-19T12:50:54.336474800Z"
    }
   },
   "id": "3bd6416dbdd03a4e"
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "outputs": [],
   "source": [
    "TrainDataPath=os.path.join(DataPreProcess.ProcessedDatasetFolder,'TrainData.npy')\n",
    "ValidationDataPath=os.path.join(DataPreProcess.ProcessedDatasetFolder,'ValidationData.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:54.712277300Z",
     "start_time": "2024-03-19T12:50:54.703110500Z"
    }
   },
   "id": "48ee94d89c8060ac"
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "outputs": [],
   "source": [
    "TrainDataset=BikeRidershipDataset(TrainDataPath)\n",
    "ValidationDataset=BikeRidershipDataset(ValidationDataPath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:55.096514300Z",
     "start_time": "2024-03-19T12:50:55.050629800Z"
    }
   },
   "id": "c45fbf0f671a6403"
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "outputs": [],
   "source": [
    "TrainLoader=DataLoader(TrainDataset,batch_size=batch_size,shuffle=True)\n",
    "ValidationLoader=DataLoader(ValidationDataset,batch_size=batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:55.437290400Z",
     "start_time": "2024-03-19T12:50:55.408741500Z"
    }
   },
   "id": "8305a721bc68d73d"
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < (self.best_loss - self.delta):\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:55.781714600Z",
     "start_time": "2024-03-19T12:50:55.764226300Z"
    }
   },
   "id": "4900a10b9c95c058"
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "outputs": [],
   "source": [
    "class BikeSharingDemandModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1=64, hidden_dim_2=32,hidden_dim_3=16):\n",
    "        super(BikeSharingDemandModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:56.144418700Z",
     "start_time": "2024-03-19T12:50:56.126224300Z"
    }
   },
   "id": "cd5ca1e8064b9d2c"
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "outputs": [],
   "source": [
    "#train the model\n",
    "InputDim=TrainDataset.features.shape[1]\n",
    "model=BikeSharingDemandModel(InputDim,HiddenDim_1,HiddenDim_2)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=LearningRate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:56.532159400Z",
     "start_time": "2024-03-19T12:50:56.523265200Z"
    }
   },
   "id": "1ff33ccca6cd2ffc"
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "outputs": [
    {
     "data": {
      "text/plain": "MSELoss()"
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#send the model to the device\n",
    "device=torch.device('cuda')\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:56.902122700Z",
     "start_time": "2024-03-19T12:50:56.889129700Z"
    }
   },
   "id": "1b81711aae282f40"
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=EarlyStoppingPatience, delta=EarlyStoppingDelta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:50:57.269291200Z",
     "start_time": "2024-03-19T12:50:57.261032600Z"
    }
   },
   "id": "9cdccc5781bd4ac5"
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.012696\n",
      "Epoch: 2, Loss: 0.007999\n",
      "Epoch: 3, Loss: 0.007795\n",
      "Epoch: 4, Loss: 0.007651\n",
      "Epoch: 5, Loss: 0.007502\n",
      "Epoch: 6, Loss: 0.007495\n",
      "Epoch: 7, Loss: 0.007320\n",
      "Epoch: 8, Loss: 0.007316\n",
      "Epoch: 9, Loss: 0.007243\n",
      "Epoch: 10, Loss: 0.007128\n",
      "Epoch: 11, Loss: 0.007120\n",
      "Epoch: 12, Loss: 0.007071\n",
      "Epoch: 13, Loss: 0.007032\n",
      "Epoch: 14, Loss: 0.006890\n",
      "Epoch: 15, Loss: 0.006899\n",
      "Epoch: 16, Loss: 0.006790\n",
      "Epoch: 17, Loss: 0.006761\n",
      "Epoch: 18, Loss: 0.006730\n",
      "Epoch: 19, Loss: 0.006642\n",
      "Epoch: 20, Loss: 0.006633\n",
      "Epoch: 21, Loss: 0.006564\n",
      "Epoch: 22, Loss: 0.006498\n",
      "Epoch: 23, Loss: 0.006476\n",
      "Epoch: 24, Loss: 0.006462\n",
      "Epoch: 25, Loss: 0.006438\n",
      "Epoch: 26, Loss: 0.006382\n",
      "Epoch: 27, Loss: 0.006322\n",
      "Epoch: 28, Loss: 0.006271\n",
      "Epoch: 29, Loss: 0.006235\n",
      "Epoch: 30, Loss: 0.006166\n",
      "Epoch: 31, Loss: 0.006195\n",
      "Epoch: 32, Loss: 0.006128\n",
      "Epoch: 33, Loss: 0.006107\n",
      "Epoch: 34, Loss: 0.006067\n",
      "Epoch: 35, Loss: 0.006080\n",
      "Epoch: 36, Loss: 0.005966\n",
      "Epoch: 37, Loss: 0.006031\n",
      "Epoch: 38, Loss: 0.005997\n",
      "Epoch: 39, Loss: 0.005893\n",
      "Epoch: 40, Loss: 0.005842\n",
      "Epoch: 41, Loss: 0.005883\n",
      "Epoch: 42, Loss: 0.005812\n",
      "Epoch: 43, Loss: 0.005812\n",
      "Epoch: 44, Loss: 0.005804\n",
      "Epoch: 45, Loss: 0.005737\n",
      "Epoch: 46, Loss: 0.005713\n",
      "Epoch: 47, Loss: 0.005730\n",
      "Epoch: 48, Loss: 0.005638\n",
      "Epoch: 49, Loss: 0.005684\n",
      "Epoch: 50, Loss: 0.005598\n",
      "Epoch: 51, Loss: 0.005615\n",
      "Epoch: 52, Loss: 0.005512\n",
      "Epoch: 53, Loss: 0.005509\n",
      "Epoch: 54, Loss: 0.005543\n",
      "Epoch: 55, Loss: 0.005500\n",
      "Epoch: 56, Loss: 0.005473\n",
      "Epoch: 57, Loss: 0.005453\n",
      "Epoch: 58, Loss: 0.005409\n",
      "Epoch: 59, Loss: 0.005379\n",
      "Epoch: 60, Loss: 0.005404\n",
      "Epoch: 61, Loss: 0.005387\n",
      "Epoch: 62, Loss: 0.005383\n",
      "Epoch: 63, Loss: 0.005257\n",
      "Epoch: 64, Loss: 0.005276\n",
      "Epoch: 65, Loss: 0.005237\n",
      "Epoch: 66, Loss: 0.005263\n",
      "Epoch: 67, Loss: 0.005256\n",
      "Epoch: 68, Loss: 0.005196\n",
      "Epoch: 69, Loss: 0.005233\n",
      "Epoch: 70, Loss: 0.005161\n",
      "Epoch: 71, Loss: 0.005168\n",
      "Epoch: 72, Loss: 0.005133\n",
      "Epoch: 73, Loss: 0.005112\n",
      "Epoch: 74, Loss: 0.005111\n",
      "Epoch: 75, Loss: 0.005056\n",
      "Epoch: 76, Loss: 0.005154\n",
      "Epoch: 77, Loss: 0.005103\n",
      "Epoch: 78, Loss: 0.005058\n",
      "Epoch: 79, Loss: 0.004996\n",
      "Epoch: 80, Loss: 0.005030\n",
      "Epoch: 81, Loss: 0.005015\n",
      "Epoch: 82, Loss: 0.004969\n",
      "Epoch: 83, Loss: 0.004964\n",
      "Epoch: 84, Loss: 0.004872\n",
      "Epoch: 85, Loss: 0.004952\n",
      "Epoch: 86, Loss: 0.004969\n",
      "Epoch: 87, Loss: 0.004912\n",
      "Epoch: 88, Loss: 0.004951\n",
      "Epoch: 89, Loss: 0.004898\n",
      "Epoch: 90, Loss: 0.004874\n",
      "Epoch: 91, Loss: 0.004819\n",
      "Epoch: 92, Loss: 0.004876\n",
      "Epoch: 93, Loss: 0.004842\n",
      "Epoch: 94, Loss: 0.004846\n",
      "Epoch: 95, Loss: 0.004796\n",
      "Epoch: 96, Loss: 0.004809\n",
      "Epoch: 97, Loss: 0.004789\n",
      "Epoch: 98, Loss: 0.004789\n",
      "Epoch: 99, Loss: 0.004808\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss=0.0\n",
    "    for inputs,labels in TrainLoader:\n",
    "        inputs,labels=inputs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item()\n",
    "    print('Epoch: %d, Loss: %.6f' %(epoch+1,running_loss/len(TrainLoader)))\n",
    "    LossList.append(running_loss/len(TrainLoader))\n",
    "    early_stopping(running_loss/len(TrainLoader))\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:52:06.014743400Z",
     "start_time": "2024-03-19T12:50:57.639075400Z"
    }
   },
   "id": "8333458b2f884d5c"
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5557.4824\n",
      "Validation Loss: 4785.7710\n",
      "Validation Loss: 14304.7822\n",
      "Validation Loss: 8786.0264\n",
      "Validation Loss: 6424.7559\n",
      "Validation Loss: 9671.6875\n",
      "Validation Loss: 10910.5166\n",
      "Validation Loss: 7188.5601\n",
      "Validation Loss: 8560.7812\n",
      "Validation Loss: 5479.1353\n",
      "Validation Loss: 3210.2861\n",
      "Validation Loss: 13911.1641\n",
      "Validation Loss: 5863.5801\n",
      "Validation Loss: 3984.9365\n",
      "Validation Loss: 7166.1699\n",
      "Validation Loss: 3986.6750\n",
      "Average Test Loss: 7487.0194\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in ValidationLoader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        predictions = model(inputs)\n",
    "        predictions = predictions.to(device)\n",
    "        predictions=predictions.cpu().numpy()\n",
    "        targets=targets.cpu().numpy()\n",
    "        predictions=ValidationDataset.scaler.inverse_transform(predictions)\n",
    "        targets=ValidationDataset.scaler.inverse_transform(targets)\n",
    "        loss = criterion(torch.from_numpy(predictions), torch.from_numpy(targets))\n",
    "        ValidationLossList.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        print(f'Validation Loss: {loss:.4f}')\n",
    "    avg_loss = total_loss / len(ValidationLoader)\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T12:52:06.052496300Z",
     "start_time": "2024-03-19T12:52:06.016744300Z"
    }
   },
   "id": "7435183afbd498f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dd92ddb6f57102b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
